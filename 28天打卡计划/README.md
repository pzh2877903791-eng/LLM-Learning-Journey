# 28 天打卡计划（阶段 1：从零跑到本地 7B 大模型）🚀
# 28 天打卡计划（阶段 1：从零跑到本地 7B 大模型 - 优化版）🚀

**目标**：28 天后能在自己电脑流畅跟 Qwen2-7B 聊天  
**目标**：28 天后能在自己电脑流畅跟 Qwen2-7B **或 Qwen2-3B** 聊天  
**开始日期**：2025-12-01  
**当前进度**：1/28 天

| 周 | 天 | 任务                                   | 状态 | 验证标准                 | 提交记录 |
|----|----|----------------------------------------|------|--------------------------|----------|
| W1 | 1  | 装好 WSL2 或 Git Bash，学会 cd/ls/mkdir/rm |  ✅   | 截图 `pwd && ls -la`     |     2025.12.1     |
| W1 | 2  | Python 虚拟环境 + pip + numpy/pandas   | ☐    | 跑通 pandas 小脚本       |          |
| W1 | 3  | Git 基础（clone/add/commit/push）      | ☐    | 今日代码 push 到本仓库   |          |
| W1 | 4-7| 每天刷 5 道 LeetCode 简单题            | ☐    | 周日完成 20 道           |          |
| W2 | 8-9| 安装 PyTorch 2.3+，跑官方教程          | ☐    | `torch.cuda.is_available()`=True |          |
| W2 | 10 | 手敲 4 层 Transformer                  | ☐    | loss 下降曲线截图        |          |
| W2 | 11-14| 跑通 nanoGPT（莎士比亚）             | ☐    | 训练日志                 |          |
| W3 | 15-20| Hugging Face Course + pipeline 实战  | ☐    | 4 个小 Demo              |          |
| W4 | 21-23| 用 Ollama / LM Studio 跑通 qwen2:7b  | ☐    | 聊天截图                 |          |
| W4 | 24-25| transformers 本地加载 Qwen2-7B（4bit）| ☐    | 运行 test_qwen2.py       |          |
| W4 | 26-27| 装 OpenWebUI，接通本地模型             | ☐    | Web 界面截图             |          |
| W4 | 28 | 毕业！整理所有截图、代码               | ☐    | 本文件夹完整             | 🎉       |

**每日打卡规则**：  
学完一项 → 在对应文件夹里新建 `dayX-任务名` 文件夹 → 放代码 + 截图 → commit 消息写 “Day X done”

28 天后这里就是你最硬核的 LLM 学习简历！

Made with Grok (xAI)
**当前进度**：0/28 天  
**硬件状态**：✅ 内存 31.5GB | ❌ 无 NVIDIA GPU | ⚠️ 需 CPU 优化

## 学习路径说明
**根据你的硬件配置优化**：
- 前 2 周：打牢基础，理解原理
- 第 3 周：学习 CPU 优化技术
- 第 4 周：部署优化版模型，实现流畅对话

---

### **W1 基础夯实**
| 天 | 任务 | 状态 | 验证标准 | 提交记录 |
|----|------|------|----------|----------|
| 1 | 装好 Git Bash，学会 cd/ls/mkdir/rm | ✅ | 截图 `pwd && ls -la` |2025.12.1 |
| 2 | Python 虚拟环境 + pip + numpy | ☐ | 跑通 numpy 小脚本 | |
| 3 | Git 基础（clone/add/commit/push - **HTTPS优先**） | ☐ | 今日代码 push 到本仓库 | |
| 4 | PyTorch 张量操作基础 | ☐ | 完成张量运算练习 | |
| 5 | 自动求导与简单神经网络 | ☐ | 实现 2 层 MLP | |
| 6 | 第一个完整训练循环 | ☐ | MNIST 分类训练日志 | |
| 7 | 周末整合与复习 | ☐ | Week 1 总结文档 | |

### **W2 核心原理**
| 天 | 任务 | 状态 | 验证标准 | 提交记录 |
|----|------|------|----------|----------|
| 8 | 神经网络原理 + CNN 基础 | ☐ | CIFAR-10 分类脚本 | |
| 9 | RNN/LSTM 原理理解 | ☐ | 文本生成小 demo | |
| 10 | **Transformer 原理精讲** | ☐ | 图解注意力机制笔记 | |
| 11 | 实现 Self-Attention 机制 | ☐ | 单头注意力代码 | |
| 12 | 实现单层 Transformer Block | ☐ | Transformer 层代码 | |
| 13 | Hugging Face Transformers 入门 | ☐ | 跑通 3 个 pipeline demo | |
| 14 | 周末项目：微调小型 BERT | ☐ | 情感分析模型 | |

### **W3 优化与部署**
| 天 | 任务 | 状态 | 验证标准 | 提交记录 |
|----|------|------|----------|----------|
| 15 | **模型量化原理**（4bit/8bit） | ☐ | 量化对比实验报告 | |
| 16 | **CPU 优化技术**（GGUF/llama.cpp） | ☐ | CPU 推理速度测试 | |
| 17 | Ollama 安装 + 小模型测试 | ☐ | 跑通 llama2:7b（如果内存允许） | |
| 18 | LM Studio CPU 优化配置 | ☐ | 本地模型加载成功 | |
| 19 | **小模型实战**：Qwen2-1.8B CPU 部署 | ☐ | 1.8B 模型对话截图 | |
| 20 | **升级测试**：Qwen2-3B CPU 部署 | ☐ | 3B 模型响应时间测试 | |
| 21 | API 服务部署（FastAPI + 本地模型） | ☐ | 本地 API 调用成功 | |

### **W4 毕业冲刺**
| 天 | 任务 | 状态 | 验证标准 | 提交记录 |
|----|------|------|----------|----------|
| 22 | **挑战目标**：Qwen2-7B CPU 加载测试 | ☐ | 7B 模型加载成功截图 | |
| 23 | CPU 推理优化调参 | ☐ | 响应时间优化报告 | |
| 24 | Open WebUI 安装配置 | ☐ | Web 界面访问成功 | |
| 25 | 连接本地模型到 WebUI | ☐ | WebUI 对话截图 | |
| 26 | 性能测试与瓶颈分析 | ☐ | 性能测试报告 | |
| 27 | 学习总结与代码整理 | ☐ | 完整项目文档 | |
| 28 | **毕业！** 项目展示与简历更新 | ✅ | 本文件夹完整 | 🎉 |

---

## 📝 **每日打卡规则**
1. **学完一项** → 在对应文件夹里新建 `dayX-任务名` 文件夹
2. **放代码 + 截图** → 关键代码和运行结果截图
3. **Commit** → 消息写 “Day X done”
4. **每日反思** → 简单记录学习心得和遇到的问题

## ⚠️ **重要调整说明**
1. **原计划 LeetCode** → 替换为更相关的 **PyTorch 基础练习**
2. **原计划 "手敲 4 层 Transformer"** → 拆解为 **原理→单层→完整** 三步
3. **GPU 依赖任务** → 调整为 **CPU 优化技术学习**
4. **7B 模型目标** → 根据实际情况调整为 **1.8B/3B/7B 渐进式挑战**

## 💡 **硬件优化提示**
- 运行大模型前：关闭不必要的程序，释放内存
- 使用 **GGUF 格式** 模型获得最佳 CPU 性能
- 考虑设置 **虚拟内存**（页面文件）辅助大模型加载
- 如体验不佳，可用 **Google Colab** 作为 GPU 备用方案

---

**28 天后这里就是你最硬核的 LLM 学习简历！**  
*计划根据你的 Intel CPU + 32GB 内存配置优化，兼顾学习深度与实践可行性*
